{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"train.pkl\")\n",
    "df_test = pd.read_pickle(\"test.pkl\")\n",
    "\n",
    "df.rename(columns={\"PM2.5_target\": \"y\"}, inplace=True)\n",
    "df_test.rename(columns={\"PM2.5_target\": \"y\"}, inplace=True)\n",
    "\n",
    "df.rename(columns={c: c.replace(\" \", \"_\") for c in df.columns}, inplace=True)\n",
    "df_test.rename(columns={c: c.replace(\" \", \"_\") for c in df_test.columns}, inplace=True)\n",
    "\n",
    "# Keep only data from 2018 onwards\n",
    "df = df.loc[df.index.get_level_values(\"date\") >= \"2018-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year, month, day, hour columns (as dummies)\n",
    "df[\"year\"] = df.index.get_level_values(\"date\").year\n",
    "df[\"month\"] = df.index.get_level_values(\"date\").month\n",
    "df[\"day\"] = df.index.get_level_values(\"date\").day\n",
    "df[\"hour\"] = df.index.get_level_values(\"date\").hour\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"month\", \"day\", \"hour\"], drop_first=True)\n",
    "\n",
    "# Drop columns with only NaNs\n",
    "df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "# In case the first day is missing, just take the average over all cities\n",
    "day1 = df.index.get_level_values(\"date\") == df.index.get_level_values(\"date\").min()\n",
    "df.loc[day1, \"y\"] = df.loc[day1, \"y\"].replace({np.nan: df.loc[day1, \"y\"].mean()})\n",
    "\n",
    "# Fill any missings with the past value (by city)\n",
    "df[\"y\"] = df.groupby(\"city\").y.ffill()\n",
    "# \n",
    "# \n",
    "# \n",
    "# Add year, month, day, hour columns (as dummies)\n",
    "df_test[\"year\"] = df_test.index.get_level_values(\"date\").year\n",
    "df_test[\"month\"] = df_test.index.get_level_values(\"date\").month\n",
    "df_test[\"day\"] = df_test.index.get_level_values(\"date\").day\n",
    "df_test[\"hour\"] = df_test.index.get_level_values(\"date\").hour\n",
    "# \n",
    "df_test = pd.get_dummies(df_test, columns=[\"month\", \"day\", \"hour\"], drop_first=True)\n",
    "# \n",
    "df_test = df_test[list(set(df.columns).intersection(set(df_test.columns)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logarithm of target\n",
    "df[\"logy\"] = np.log(df[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "THC       0.965903\n",
       "HCHO      0.940675\n",
       "Hg        0.939259\n",
       "MH        0.935458\n",
       "CH4       0.818199\n",
       "            ...   \n",
       "day_5     0.000000\n",
       "day_4     0.000000\n",
       "day_3     0.000000\n",
       "day_2     0.000000\n",
       "day_10    0.000000\n",
       "Length: 97, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values per column\n",
    "df.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively, fill features using LightGBM trained on all features that have no NaNs\n",
    "features = df.drop(columns=[\"y\", \"PM2.5\", \"logy\"]).isna().mean().sort_values()\n",
    "features_full = features[features == 0].index.tolist()\n",
    "features_to_impute = features[features > 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:29<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "for feature in tqdm(features_to_impute, total=len(features_to_impute)):\n",
    "    # Make LGBM data\n",
    "    X = df[features_full]\n",
    "    X_test = df_test[features_full]\n",
    "    y = df[feature]\n",
    "\n",
    "    # Train LGBM\n",
    "    lgbm = LGBMRegressor(verbose=0, n_jobs=-1).fit(X, y)\n",
    "    y_pred = lgbm.predict(X)\n",
    "    y_pred_test = lgbm.predict(X_test)\n",
    "\n",
    "    # Fill NaNs\n",
    "    df[feature].fillna(pd.Series(y_pred, index=df.index), inplace=True)\n",
    "    df_test[feature].fillna(pd.Series(y_pred_test, index=df_test.index), inplace=True)\n",
    "\n",
    "    # We can now use this feature for imputing other features\n",
    "    features_full.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c not in [\"y\", \"PM2.5\", \"logy\"]]\n",
    "out1 = [\"y\"]\n",
    "out2 = [\"logy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[features]\n",
    "X_test = df_test[features]\n",
    "y = df[out2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM Predictions\n",
    "\n",
    "### 1. Naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM on all features\n",
    "lgbm = LGBMRegressor(verbose=0).fit(X_train, y)\n",
    "y_pred = np.exp(lgbm.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(y_pred, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_pickle(\"00.pkl\") # LGBM prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wide Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if instead, we use the contemporaneous values of other cities to help in prediction?\n",
    "# Transform dataframe to wide format, using index level city\n",
    "df_wide = df.reset_index().pivot(index=\"date\", columns=\"city\", values=features + out2)\n",
    "df_wide.columns = df_wide.columns.map('_'.join).str.strip('_')\n",
    "df_wide_test = df_test.reset_index().pivot(index=\"date\", columns=\"city\", values=features)\n",
    "df_wide_test.columns = df_wide_test.columns.map('_'.join).str.strip('_')\n",
    "\n",
    "features = [c for c in df_wide.columns if not (c.startswith(\"y_\") or c.startswith(\"logy_\"))]\n",
    "\n",
    "df_wide = df_wide.astype(float)\n",
    "df_wide_test = df_wide_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = sorted(df.index.get_level_values(\"city\").unique().to_list())\n",
    "out2 = [\"logy_\" + city for city in cities]\n",
    "pred = pd.DataFrame(index=df_test.index, columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models: 100%|██████████| 12/12 [00:17<00:00,  1.44s/city]\n"
     ]
    }
   ],
   "source": [
    "for city in tqdm(cities, total=len(cities), unit=\"city\", desc=\"Fitting models\"):\n",
    "    # Train LGBM\n",
    "    lgbm = LGBMRegressor(verbose=0).fit(df_wide[features], df_wide[f\"logy_{city}\"])\n",
    "    y_pred = lgbm.predict(df_wide_test[features])\n",
    "    pred.loc[pred.index.get_level_values(\"city\") == city, \"y\"] = np.exp(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_pickle(\"00_wide.pkl\") # LGBM prediction using contemporaneous values of other cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMDataset(Dataset):\n",
    "    # We could also define our constructor to take a pandas dataframe and extract the \n",
    "    # features / targets directly. Notice that we must also specify a sequence length\n",
    "    def __init__(self, X, y, seq_len, enforce_shape=False):\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.Tensor(y)\n",
    "        self.added = 0\n",
    "        # We need the Tensors to be of a length divisible by the sequence length\n",
    "        if enforce_shape and (X.shape[0] % seq_len != 0):\n",
    "            # Pad with zeros\n",
    "            self.X = torch.cat([self.X, torch.zeros(seq_len - X.shape[0] % seq_len, *self.X.shape[1:])])\n",
    "            self.y = torch.cat([self.y, torch.zeros(seq_len - y.shape[0] % seq_len, *self.y.shape[1:])])\n",
    "            self.added = seq_len - X.shape[0] % seq_len\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.y.shape[0] - self.seq_len) // self.seq_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx *= self.seq_len\n",
    "        return self.X[idx:idx+self.seq_len], self.y[idx:idx+self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs=1, num_lstm_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.lstm = nn.LSTM(n_inputs, n_hidden, num_lstm_layers, batch_first=True)\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_preds(df_train, df_test, batch_size=128, seq_len=96, n_epochs=50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "    # Use mean-square error as a loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    preds = {}\n",
    "    \n",
    "    for city in tqdm(cities, total=len(cities), unit=\"city\", desc=\"Fitting models\"):\n",
    "        dfx = df_train.xs(city, level=\"city\").copy()\n",
    "        dfy = df_test.xs(city, level=\"city\").copy()\n",
    "\n",
    "        X_train = dfx.drop(columns=[\"y\", \"PM2.5\", \"logy\"])\n",
    "        y_train = dfx[\"logy\"].values.reshape(-1, 1)\n",
    "\n",
    "        features = X_train.columns\n",
    "\n",
    "        X_test = dfy[features]\n",
    "\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        dsx = PMDataset(X=X_train, y=y_train, seq_len=seq_len)\n",
    "        dsy = PMDataset(X=X_test, y=np.zeros(X_test.shape[0]), seq_len=seq_len, enforce_shape=True)\n",
    "        dlx = DataLoader(dsx, batch_size=batch_size, shuffle=True)\n",
    "        dly = DataLoader(dsy, batch_size=1, shuffle=False)\n",
    "\n",
    "        nnet = SimpleLSTM(X_train.shape[1], 128, 1).to(device)\n",
    "\n",
    "        opt = optim.Adam(nnet.parameters(), lr=1e-4)\n",
    "        sched = lr_scheduler.OneCycleLR(opt, max_lr=0.01, \n",
    "                                        steps_per_epoch=len(dlx), \n",
    "                                        epochs=n_epochs) \n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            nnet.train()\n",
    "\n",
    "            for X, y in dlx:\n",
    "                # Pass the batch to the GPU\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Reset the gradients\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # Compute the forward pass\n",
    "                y_pred = nnet(X)\n",
    "                \n",
    "                # Compute the loss\n",
    "                loss = loss_fn(y, y_pred)\n",
    "\n",
    "                # Compute the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters\n",
    "                opt.step()\n",
    "                sched.step()\n",
    "\n",
    "        # Make predictions\n",
    "        nnet.eval()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in dly:\n",
    "                X = X.to(device)\n",
    "                y_pred.append(nnet(X).cpu().numpy())\n",
    "\n",
    "        y_pred = np.concatenate(y_pred).reshape(-1)[:-dsy.added]\n",
    "\n",
    "        preds[city] = np.exp(y_pred)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models: 100%|██████████| 12/12 [00:14<00:00,  1.20s/city]\n"
     ]
    }
   ],
   "source": [
    "preds = make_lstm_preds(df, df_test)\n",
    "preds = pd.DataFrame(preds, index=df_test.index.get_level_values(\"date\").unique())\n",
    "preds = pd.melt(preds, ignore_index=False).rename(columns={\"variable\": \"city\", \"value\": \"y\"})\n",
    "preds[\"date\"] = preds.index\n",
    "preds = preds.set_index([\"date\", \"city\"]).sort_index(level=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_pickle(\"00_lstm.pkl\") # LSTM prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
