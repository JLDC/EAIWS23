{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders\n",
    "\n",
    "In order to efficiently train a neural network using batched data, we will use two of [PyTorch's data utilities](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html): the `Dataset` and the `DataLoader` classes.\n",
    "\n",
    "PyTorch (or other frameworks / packages) provide several well-known datasets, such as MNIST, CIFAR-10, Fashion-MNIST, etc. However, we are interested in creating our own custom dataset based on the data we have.\n",
    "\n",
    "Once the `Dataset` is created, we can pass it to a `DataLoader` object. The `DataLoader` object wraps around the `Dataset` and provides us with the ability to iterate over the dataset in batches. This is especially useful when the dataset is large and cannot fit into memory. The `DataLoader` also provides other utilities such as shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Interface of a Dataset\n",
    "\n",
    "In order to create a custom dataset, we need to create a class that inherits from the `Dataset` class and implements the following methods:\n",
    "\n",
    "- `__init__`: The constructor method where we can read in any data, such as a CSV file.\n",
    "- `__len__`: The method that returns the length of the dataset.\n",
    "- `__getitem__`: The method that returns a sample from the dataset given an index.\n",
    "\n",
    "Once these three methods are implemented, we can instantiate the class, pass it to a `DataLoader` object and start iterating over it to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object that inherits from the Dataset class\n",
    "class ExampleDataset(Dataset): \n",
    "    # Constructor\n",
    "    def __init__(self, X, y):\n",
    "        # Store an X and y (arbitrary choice, we could store different things,\n",
    "        # here our X will be the features/input and y the target/output)\n",
    "        self.X = torch.Tensor(X)  \n",
    "        self.y = torch.Tensor(y) \n",
    "        # Transforming to tensor at construction instead of indexing will reduce \n",
    "        # overhead when iterating over the dataset but might require more memory\n",
    "    \n",
    "    # Length method (len) returns the number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0] # Number of observations given our y is NxK (K outputs, N observations)\n",
    "    \n",
    "    # Get item method (getitem) returns the i-th sample of the dataset\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i] # Return the i-th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some example data\n",
    "X = np.arange(12).reshape(-1, 3) # 4x3 matrix of features (4 observations, 3 features)\n",
    "y = np.arange(4).reshape(-1, 1) # 4x1 matrix of targets (4 observations, 1 target)\n",
    "\n",
    "# Create a dataset object\n",
    "dataset = ExampleDataset(X, y)\n",
    "\n",
    "# Create a dataloader object\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}:\\n\")\n",
    "    print(f\" X:\\n{x}\")\n",
    "    print(f\" y:\\n{y}\")\n",
    "    print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two important things from the code above:\n",
    "\n",
    "+ Each batch has 2 elements. This is given by the `batch_size` parameter in the `DataLoader` object. Consider what happens when the `batch_size` does not divide the length of the dataset. For example, if we set the `batch_size` to 3, then the last batch will have only 1 element. This is not a problem and the `DataLoader` handles this case automatically. However, it is important to keep this in mind when writing the training code.\n",
    "+ The batches are ordered the way our data was created. This is because we set `shuffle` to `False`. If we set `shuffle` to `True`, then the `DataLoader` will shuffle the batches and we will get different orders of data in each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Considerations for RNNs\n",
    "\n",
    "Datasets and data loaders are useful for all kinds of neural networks. However, we should keep in mind, that different types of networks will expect different input shapes. For instance, a feedforward neural network expects a batch of input to have 2 dimensions: (`batch_size`, `input_size`). A recurrent neural network, on the other hand, expects its batched input to have 3 dimensions: (`batch_size`, `sequence_length`, `input_size`) if we set the parameter `batch_first` to `True` in the RNN's constructor. Otherwise, the input shape will be (`sequence_length`, `batch_size`, `input_size`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and Sequence Length\n",
    "\n",
    "Suppose we have 100 observations $x_1, x_2, \\dots, x_{100}$. If we were to batch this data for a FFNN, we would have no troubles at all, suppose we would like batches of size $B=5$, we simply partition the data into $\\lceil 100/B \\rceil$ non-overlapping chunks, e.g., $\\{x_1, x_2, x_3, x_4, x_5\\}, \\{x_6, x_7, x_8, x_9, x_{10}\\}, \\dots, \\{x_{96}, x_{97}, x_{98}, x_{99}, x_{100}\\}$.\n",
    "\n",
    "Unfortunately, we want to work with models that can handle sequential data, and so we need to be a bit more careful. Indeed, our models don't only take an observation as input but rather a **sequence of observations**. Hence if we choose $B=5$, we should have 5 sequences of observations per batch, not just 5 observations! But what is the length of each sequence? Well, this is a design choice, but we can choose to have all sequences of the same length, e.g., $L=10$. This means that we will have 10 observations per sequence, and so we will have $L \\cdot B = 50$ observations per batch. Hence, we would have only two batches instead of the 20 we had in the FFNN case.\n",
    "\n",
    "#### FFNN Batching\n",
    "Batch 1: $\\{x_1, x_2, x_3, x_4, x_5\\}$  \n",
    "Batch 2: $\\{x_6, x_7, x_8, x_9, x_{10}\\}$  \n",
    "$\\dots$  \n",
    "Batch 20: $\\{x_{96}, x_{97}, x_{98}, x_{99}, x_{100}\\}$\n",
    "\n",
    "\n",
    "#### RNN Batching\n",
    "Batch 1: $\\{(x_1, x_2, x_3, x_4, x_5), (x_6, x_7, x_8, x_9, x_{10}), \\dots (x_{45}, x_{46}, x_{47}, x_{48}, x_{49}, x_{50})\\}$  \n",
    "Batch 2: $\\{(x_{51}, x_{52}, x_{53}, x_{54}, x_{55}), \\dots (x_{95}, x_{96}, x_{97}, x_{98}, x_{99}, x_{100})\\}$\n",
    "\n",
    "#### Pause and Ponder\n",
    "What is stopping us from having overlapping sequences? E.g., the first batch could be $\\{(x_1, x_2, x_3, x_4, x_5), (x_2, x_3, x_4, x_5, x_6), \\dots (x_6, x_7, x_8, x_9 x_{10})\\}$. This would leave us with 19 batches, so are we just *losing data* by not doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching PM2.5 Data\n",
    "\n",
    "Let us make things a bit more concrete and look at how to batch the data of the predictive challenge. To keep things simple, we will focus our example on the observations in Delhi only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only observations from Delhi\n",
    "df = pd.read_pickle(\"train.pkl\")\n",
    "df = df.xs(\"Delhi\", level=\"city\")\n",
    "# Keep only the first 5 columns to make it more legible\n",
    "df = df[df.columns[:5]]\n",
    "# Avoid NaNs later, just to showcase that data loading in practice later on\n",
    "df.replace({np.nan: 0}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDataset(Dataset):\n",
    "    # We could also define our constructor to take a pandas dataframe and extract the \n",
    "    # features / targets directly. Notice that we must also specify a sequence length\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.Tensor(y)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0] - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx:idx+self.seq_len], self.y[idx:idx+self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset based on our training data\n",
    "dataset = CityDataset(\n",
    "    X=df.drop(\"PM2.5_target\", axis=1).values,\n",
    "    y=df[[\"PM2.5_target\"]].values,\n",
    "    seq_len=24\n",
    ")\n",
    "\n",
    "# Create a dataloader, loading 32 batches of sequences at a time\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how one single batch looks like...\n",
    "for x, y in dataloader:\n",
    "    print(\"First Batch:\")\n",
    "    print(f\" X shape: {x.shape}\")\n",
    "    print(f\" Y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Finally, now that we have specified how our data should be loaded, we can turn to a simple example of training a neural network using this particular dataloader. The way we built our dataloader implies that the first dimension is the batch size. Hence, we have to specify `batch_first=True` when instantiating the recurrent layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recurrent network \n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden):\n",
    "        super().__init__()\n",
    "        # Use a Gated Recurrent Unit (beware of the batch_first parameter!)\n",
    "        self.rnn = nn.GRU(n_inputs, n_hidden, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, n_outputs)\n",
    "\n",
    "    # Define the forward pass to remove the hidden state from the output\n",
    "    def forward(self, x):\n",
    "        # Apply the RNN\n",
    "        x, _ = self.rnn(x) # (output, hidden state), discard the hidden state\n",
    "        # Apply the linear layer\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate an RNN with 4 inputs, 1 output and 32 hidden nodes\n",
    "gru = MyRNN(4, 1, 32).to(device)\n",
    "\n",
    "# Use mean-square error as a loss\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Instantiate Adam optimizer with learning rate 0.0001\n",
    "opt = optim.Adam(gru.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the typical training structure doesn't change\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "losses = np.zeros(n_epochs)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training Network...\", unit=\"epoch\"):\n",
    "    # Within each epoch, we iterate over the batches\n",
    "    for X, y in dataloader:\n",
    "        # Pass the batch to the GPU\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Compute the forward pass\n",
    "        y_pred = gru(X)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y, y_pred)\n",
    "        \n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        opt.step()\n",
    "\n",
    "        # Store the epoch average losses to plot later on\n",
    "        losses[epoch] += loss.detach().numpy() / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(losses, \"-o\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE (on train set)\")\n",
    "ax.grid(alpha=.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
