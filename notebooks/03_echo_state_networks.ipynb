{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%matplotlib notebook\n",
    "from ipywidgets import interact, widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo State Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to move out attention to the problem of modeling (and estimating) **dynamic** nonlinear models, such as the one defined by nonlinear time series or different equations.\n",
    "\n",
    "The tool we are going to discuss is the so call **Echo State Network (ESN)**, which in a nutshell is a class of nonlinear models with a state-space structure that is constructed entirely out of random functions - a bit like in the RWNN setup we just discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Echo State Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESNs are constructed starting from the following general model equations:\n",
    "$$ \\begin{align}\n",
    "x_t & = H(x_{t-1}, z_t), \\\\\n",
    "y_t & = F(x_t),\n",
    "\\end{align}\n",
    "$$\n",
    "where here we use $x_t \\in \\mathbb{R}^{d_x}$, $y_t \\in \\mathbb{R}^{d_y}$ and $z_t \\in \\mathbb{R}^{d_z}$ to signify time series that do not necessarily need to be noisy. The sequence $\\{ z_t \\}_{t=1}^T$ is called the **inputs**, while sequence $\\{ y_t \\}_{t=1}^T$ is called the **targets** of the model.\n",
    "\n",
    "An ESN proposes a specific form for (1)-(2), namely\n",
    "$$ \\begin{align}\n",
    "x_t & = \\alpha x_{t-1} + (1 -\\alpha)\\sigma(A x_{t-1} + C z_t + \\zeta), \\\\\n",
    "y_t & = W x_t, \n",
    "\\end{align}\n",
    "$$\n",
    "where $A \\in \\mathbb{R}^{d_x \\times d_x}$, $C \\in \\mathbb{R}^{d_x \\times d_z}$ and $\\zeta \\in \\mathbb{R}^{d_x}$ are *randomly drawn matrices/vectors*; $\\alpha \\in [0,1]$ is the so-called *leak-rate*, and $W \\in \\mathbb{R}^{d_y \\times d_x}$ are the coefficients of the model that are actually estimated from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESN models has show to be effective in reconstructing even complex systems and dynamics, and their expressive power is coupled with an increadible easy of implementation. Since all of the ESN parameters in equation (1) do not need estimation, we can generally proceed as follows:\n",
    "\n",
    "1. Randomly draw $A$, $C$ and $\\zeta$ from a chosen distribution, select $\\alpha$.\n",
    "2. Given inputs $\\{ z_t \\}_{t=1}^T$, *collect* the ESN states $\\{ x_t \\}_{t=1}^T$ by fixing an initial state $x_0$ and iterating\n",
    "    $$ x_t = \\alpha x_{t-1} + (1 -\\alpha)\\sigma(A x_{t-1} + C z_t + \\zeta) .$$ \n",
    "3. Construct target matrix $Y := (y_1', \\ldots, y_T')' \\in \\mathbb{R}^{T \\times d_y}$ and state matrix $X := (x_1, \\ldots, x_T)\\in \\mathbb{R}^{T \\times d_x}$.\n",
    "4. Use ridge regression with penalty $\\lambda \\geq 0$ to estimate \n",
    "    $$ \\widehat{W} := (Z' Z + \\lambda I)^{-1} Z'Y . $$\n",
    "\n",
    "Let us write this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE: state collection\n",
    "def esn_collect(Z, A, C, zeta, alpha):\n",
    "    T = Z.shape[0]\n",
    "    Dx = A.shape[0]\n",
    "    # NOTE: we are going to assume that x_0 == 0 for simplicity\n",
    "    #       and that the nonlinear map \\sigma(.) is tanh(.)\n",
    "    X = np.zeros([T, Dx])\n",
    "    X[0,:] = (1-alpha) * np.tanh(C @ Z[1,:] + zeta)\n",
    "    for t in range(1,T):\n",
    "        X[t,:] = alpha * X[t-1,:] + (1-alpha) * np.tanh(A @ X[t-1,:] + C @ Z[t,:] + zeta)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE: esn fitting\n",
    "def esn_fit(Y, Z, L, A, C, zeta, alpha):\n",
    "    # collect states\n",
    "    X = esn_collect(Z, A, C, zeta, alpha)\n",
    "     # ridge regression with penalty L\n",
    "    # NOTE: we adjust here by T, the time length, so that if we increase\n",
    "    #       the sample size, the penalization remains comparable\n",
    "    T, K = X.shape\n",
    "    W_hat = np.linalg.solve(np.dot(X.T, X) + (T*L)*np.eye(K), np.dot(X.T, Y))\n",
    "    # compute fitted values and fit error\n",
    "    Y_fit = X @ W_hat\n",
    "    Error = Y - Y_fit\n",
    "    return (W_hat, X, Y_fit, Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try and fit an nonlinear deterministic sequence using the ESN model. For this example, let us consider a simple but not totally trivial model where\n",
    "$$\\begin{align}\n",
    "z_t & = sin(t) , \\\\\n",
    "y_t & = -0.3 + z_t + z_t^2 + 0.3\\, cos(3 z_t - 3) . \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the following code, we randomly sample the entried in the parameter matrices $A$ and $C$ from a standard normal distribution, while $\\zeta$ has entries from a uniform distribution over $[-1,1]$.\n",
    "We are also able to tweak the ridge penalty, $\\lambda$ (slider `Lambda`), and see how this affects the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esn_plot(T = 100, Dx = 10, rho = 0.5, gamma = 1, alpha = 0.1, Lambda = 1e-7):\n",
    "    # generate data\n",
    "    time = np.linspace(0, T*0.1, T)\n",
    "    #print(time)\n",
    "    Z = np.sin(time)[:,None]\n",
    "    Y = Z + Z**2 - 0.3 + 0.3*np.cos(3*(Z - 1))\n",
    "\n",
    "    # generate ESN parameters\n",
    "    rng = np.random.default_rng(75871685)\n",
    "    A = rng.normal(loc=0, scale=1, size=[Dx, Dx])\n",
    "    C = rng.normal(loc=0, scale=1, size=[Dx, 1])\n",
    "    zeta = rng.uniform(low=-1, high=1, size=Dx)\n",
    "\n",
    "    # normalize\n",
    "    A = (A / np.linalg.norm(A, 2)) * rho\n",
    "    C = (C / np.linalg.norm(C, 2)) * gamma\n",
    "\n",
    "    # fit esn\n",
    "    _, X, Y_fit, _ = esn_fit(Y, Z, Lambda, A, C, zeta, alpha)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(3, 1, 1)\n",
    "    ax.plot(time, X)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN States\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    ax.plot(time, Z, c=\"0.6\", zorder=1)\n",
    "    ax.plot(time, Y, c=\"C1\", zorder=2)\n",
    "    ax.plot(time, Y_fit, c=\"C2\", zorder=3)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN Fit\")\n",
    "    ax.legend([\"$z_t$\", \"$y_t$\", \"ESN\"])\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(esn_plot, T=(1, 500), Dx=(5,50), rho=(0.1, 1.1), gamma=(0.1,3), alpha=(0.,1.),\n",
    "            Lambda=widgets.FloatLogSlider(min=-8, max=2, value=1e-6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, as we increase the dimension of state vector $x_t$ (changing slider `Dx`), we can see that the fit impoves to the point where it is hard to distinguish it from the true values of $y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autonomous ESN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback from the previous ESN example is that often we do not only want to *fit* the data, but we would also like to *reconstruct the underlying dynamics*. That is, we would like the ESN to not only perfom well at a one-shot (one-step) forecasting task, but be able to reproduce the time series of interest directly.\n",
    "We can ask this of the ESN model because it is state-space-based and, therefore, if we can iterate the states *autonomously* then we should also recover the true data-generating process.\n",
    "\n",
    "The theory of ESNs fortunately guarantees that (in an appropriate sense) if the state space that $x_t$ belongs to is sufficiently big, then we will be reconstructing the underlying dynamics correctly. From a theory point of view, this is a very important result. And from the applied point of view, it is actually something we can easily implement.\n",
    "\n",
    "Recall the ESN equations\n",
    "$$ \\begin{align}\n",
    "x_t & = \\alpha x_{t-1} + (1 -\\alpha)\\sigma(A x_{t-1} + C z_t + \\zeta), \\\\\n",
    "y_t & = W x_t, \n",
    "\\end{align}\n",
    "$$\n",
    "and suppose now that we let $y_t = z_{t+1}$. That is, we impose a prective structure on our ESN model. This means that $y_{t-1} = z_t$.\n",
    "\n",
    "Now, plug in equation (2) shifted to time $t-1$ - that is, $z_t = y_{t-1} = W x_{t-1}$ - into (1), and we obtain\n",
    "$$ \\begin{align*}\n",
    "x_t & = \\alpha x_{t-1} + (1 -\\alpha)\\sigma(A x_{t-1} + C z_t + \\zeta) \\\\\n",
    "    & = \\alpha x_{t-1} + (1 -\\alpha)\\sigma(A x_{t-1} + C W x_{t-1} + \\zeta) \\\\\n",
    "    & = \\alpha x_{t-1} + (1 -\\alpha)\\sigma\\left((A + C W) x_{t-1} + \\zeta\\right) .\n",
    "\\end{align*} \n",
    "$$\n",
    "Notice that the last line provides a recursion of the form $x_t = G(x_{t-1})$, where is $G(\\cdot)$ is parametrized by $(A, C, \\zeta, \\alpha, W)$ and map $\\sigma(\\cdot)$. With this recursive formulation, which is often called the **autonomous** form of the ESN, we can predict $y_t$ (thus, $z_t$) at any future horizon, potentially:\n",
    "$$ \\begin{align*}\n",
    "x_{t+h} & = G^h (x_t) =  \\underbrace{G \\circ G \\circ \\cdots \\circ G}_{h \\text{ times}}\\,(x_t) \\\\\n",
    "y_{t+h} & = W x_{t+h}, \n",
    "\\end{align*}\n",
    "$$\n",
    "for any $h \\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the autonomous ESN iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE: autonomous iteration\n",
    "def esn_auto(X0, h, A, C, zeta, alpha, W):\n",
    "    Dx = A.shape[0]\n",
    "    Dy = W.shape[1]\n",
    "    # pre-compute autonomous matrix\n",
    "    R = A + C @ W.T\n",
    "    # NOTE: we are going to assume that x_0 == 0 for simplicity\n",
    "    #       and that the nonlinear map \\sigma(.) is tanh(.)\n",
    "    Xh = np.zeros([h, Dx])\n",
    "    Yh = np.zeros([h, Dy])\n",
    "    Xh[0,:] = X0\n",
    "    Yh[0,:] = X0 @ W\n",
    "    for t in range(1,h):\n",
    "        Xh[t,:] = alpha * Xh[t-1,:] + (1-alpha) * np.tanh(R @ Xh[t-1,:] + zeta)\n",
    "        Yh[t,:] = Xh[t,:] @ W\n",
    "    return(Xh, Yh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering again the deterministic time series model from the previous section, we now try and (1) fit process $\\{y_t\\}$ *directly* (i.e. it is both inputs and outputs); (2) forecast autonomously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esn_plot(T = 100, h = 20, Dx = 10, rho = 0.5, gamma = 1, alpha = 0.1, Lambda = 1e-7):\n",
    "    # generate data\n",
    "    time = np.linspace(0, (T+h)*0.1, T+h)\n",
    "    #print(time)\n",
    "    Z = np.sin(time)[:,None]\n",
    "    Y = Z + Z**2 - 0.3 + 0.3*np.cos(3*(Z - 1))\n",
    "\n",
    "    # split train and test\n",
    "    Y_train_in = Y[:T-1,:]\n",
    "    Y_train_out = Y[1:T,:]\n",
    "    Y_test = Y[T:,:]\n",
    "\n",
    "    # generate ESN parameters\n",
    "    rng = np.random.default_rng(75871685)\n",
    "    A = rng.normal(loc=0, scale=1, size=[Dx, Dx])\n",
    "    C = rng.normal(loc=0, scale=1, size=[Dx, 1])\n",
    "    zeta = rng.uniform(low=-1, high=1, size=Dx)\n",
    "\n",
    "    # normalize\n",
    "    A = (A / np.linalg.norm(A, 2)) * rho\n",
    "    C = (C / np.linalg.norm(C, 2)) * gamma\n",
    "\n",
    "    # fit esn\n",
    "    W_hat, X, Y_fit, _ = esn_fit(Y_train_out, Y_train_in, Lambda, A, C, zeta, alpha)\n",
    "\n",
    "    # autonomous esn\n",
    "    Xh, Yh = esn_auto(X[-1,:], h, A, C, zeta, alpha, W_hat)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(3, 1, 1)\n",
    "    ax.plot(time[0:T-1], X, c=\"0.6\")\n",
    "    ax.plot(time[T-1:-1], Xh)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN States\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    #ax.plot(time[0:T-1], Y_train_in, c=\"0.6\", zorder=1)\n",
    "    ax.plot(time[1:T], Y_train_out, c=\"0.6\", zorder=2)\n",
    "    ax.plot(time[1:T], Y_fit, c=\"k\", zorder=3)\n",
    "    ax.plot(time[T:], Y_test, c=\"C1\", zorder=2)\n",
    "    ax.plot(time[T:], Yh, c=\"C2\", zorder=3)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN Fit & Autonomous\")\n",
    "    ax.legend([\"$y_t$\", \"ESN Fit\", \"$y_t$ test\", \"ESN Auto\"], loc=\"center left\")\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(esn_plot, T=(1, 500), h=(1, 100), Dx=(5,50), rho=(0.1, 1.1), gamma=(0.1,3), alpha=(0.,1.),\n",
    "            Lambda=widgets.FloatLogSlider(min=-8, max=2, value=1e-6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Predicting Chaotic Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESNs have been widely applied to to prediction of nonlinear dynamical systems -- an area where they have been shown to be extremely effective. In this section, we will consider two cases\n",
    "\n",
    "1. Autonomous ESN prediction of an ODE system - the *Lorenz attractor*.\n",
    "2. Autonomous ESN prediction of a time-delay system - the *Mackey-Glass equations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Lorenz attractor** is a well-known three-dimensional system that exhibits caotic behavior. Its equations are:\n",
    "$$\\begin{align}\n",
    "\\frac{d x}{d t} & = \\sigma(y - x) , \\\\\n",
    "\\frac{d y}{d t} & = x(\\rho - z) - y , \\\\\n",
    "\\frac{d z}{d t} & = x y - \\beta z .\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The following code provides a simple implementation of the Lorenz equations (with a plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE: Lorenz attractor\n",
    "def lorenz_attractor(sigma, rho, beta, initial_state, delta_t, num_steps):\n",
    "    \"\"\"\n",
    "    Simulate the Lorenz attractor.\n",
    "\n",
    "    Parameters:\n",
    "        sigma (float): Parameter sigma\n",
    "        rho (float): Parameter rho\n",
    "        beta (float): Parameter beta\n",
    "        initial_state (tuple): Initial state (x0, y0, z0)\n",
    "        num_steps (int): Number of simulation steps\n",
    "\n",
    "    Returns:\n",
    "        Tuple of NumPy arrays (x, y, z) representing the simulated trajectory.\n",
    "    \"\"\"\n",
    "    x, y, z = initial_state\n",
    "    x_vals, y_vals, z_vals = [x], [y], [z]\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        dx = sigma * (y - x)\n",
    "        dy = x * (rho - z) - y\n",
    "        dz = x * y - beta * z\n",
    "\n",
    "        x += delta_t * dx\n",
    "        y += delta_t * dy\n",
    "        z += delta_t * dz\n",
    "\n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "        z_vals.append(z)\n",
    "\n",
    "    return np.array(x_vals), np.array(y_vals), np.array(z_vals)\n",
    "\n",
    "# Set the parameters\n",
    "sigma = 10.0\n",
    "rho = 28.0\n",
    "beta = 8.0 / 3.0\n",
    "initial_state = (1.0, 0.0, 0.0)\n",
    "\n",
    "# Simulate the Lorenz attractor\n",
    "x, y, z = lorenz_attractor(sigma, rho, beta, initial_state, 0.01, 10000)\n",
    "\n",
    "# Plot the 3D trajectory\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(x, y, z, lw=0.5)\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "ax.set_title('Lorenz Attractor')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to autonomously predict the $y$ component of the Lorenz equations with an ESN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esn_lorenz(steps = 1500, h = 500, Dx = 30, rho = 0.7, gamma = 1, alpha = 0.1, Lambda = 1e-9):\n",
    "    # generate data (rescaled by 0.1x)\n",
    "    data_x, data_y, data_z = lorenz_attractor(10.0, 28.0, 8.0/3, (1.0, 0.0, 0.0), 0.01, steps + h)\n",
    "    data = np.column_stack([data_x, data_y, data_z]) / 10.0\n",
    "\n",
    "    # split train and test\n",
    "    Y_train_in = data[:steps-1,:]\n",
    "    Y_train_out = data[1:steps,:]\n",
    "    Y_test = data[steps:,:]\n",
    "\n",
    "    # generate ESN parameters\n",
    "    rng = np.random.default_rng(75871685)\n",
    "    A = rng.normal(loc=0, scale=1, size=[Dx, Dx])\n",
    "    C = rng.normal(loc=0, scale=1, size=[Dx, 3])\n",
    "    zeta = rng.uniform(low=-1, high=1, size=Dx)\n",
    "\n",
    "    # normalize\n",
    "    A = (A / np.linalg.norm(A, 2)) * rho\n",
    "    C = (C / np.linalg.norm(C, 2)) * gamma\n",
    "\n",
    "    # fit esn\n",
    "    W_hat, X, Y_fit, _ = esn_fit(Y_train_out, Y_train_in, Lambda, A, C, zeta, alpha)\n",
    "\n",
    "    # autonomous esn\n",
    "    Xh, Yh = esn_auto(X[-1,:], h, A, C, zeta, alpha, W_hat)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(3, 1, 1)\n",
    "    ax.plot(range(0, steps-1), X, c=\"0.6\")\n",
    "    ax.plot(range(steps-1, steps-1+h), Xh)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN States\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    ax.plot(range(1, steps), Y_train_out[:,1], c=\"0.6\", zorder=2)\n",
    "    ax.plot(range(1, steps), Y_fit[:,1], c=\"k\", zorder=3)\n",
    "    ax.plot(range(steps, steps+h+1), Y_test[:,1], c=\"C1\", zorder=2)\n",
    "    ax.plot(range(steps, steps+h), Yh[:,1], c=\"C2\", zorder=3)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"Lorenz | Autonomous ESN\")\n",
    "    ax.legend([\"$y_t$\", \"ESN Fit\", \"$y_t$ test\", \"ESN Auto\"], loc=\"center left\")\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(esn_lorenz, steps=(500, 2500), h=(100, 1000), Dx=(5,50), rho=(0.1, 1.5), gamma=(0.1,3), alpha=(0.,1.),\n",
    "            Lambda=widgets.FloatLogSlider(min=-9, max=2, value=1e-7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to forecast a more complex model. The Mackey-Glass equations are time-delay differency equations are such that the time derivative depends not only on the current value, but also on lagged values of the model.\n",
    "\n",
    "We will consider the following Mackey-Glass model:\n",
    "$$ \\frac{d x(t)}{d t} = \\frac{\\beta x(t - \\tau)}{1 + x(t - \\tau)^n} - \\gamma x(t) ,$$\n",
    "where here the solution $x(t)$ is indexed by time explicitly to properly define delayed value $x(t - \\tau)$. We are going to conside the original cofficients employed in the original paper, $\\beta = 0.2$, $n = 10$ and $\\gamma = 0.1$, and initial condition $x(0) = 0.5$.\n",
    "\n",
    "The following function solves the Mackey-Glass model (plotting an example solution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE: mackey-glass equations\n",
    "def mackey_glass(beta, gamma, n, tau, a, delta_t, num_steps):\n",
    "    \"\"\"\n",
    "    Simulate the Mackey-Glass equations.\n",
    "\n",
    "    Parameters:\n",
    "        beta (float): Parameter beta\n",
    "        gamma (float): Parameter gamma\n",
    "        n (float): Parameter n\n",
    "        tau (float): Time delay parameter\n",
    "        a (float): Amplitude parameter\n",
    "        delta_t (float): Time step size\n",
    "        num_steps (int): Number of simulation steps\n",
    "\n",
    "    Returns:\n",
    "        NumPy array representing the simulated trajectory.\n",
    "    \"\"\"\n",
    "    x = np.zeros(num_steps)\n",
    "    x[0] = 0.5  # Initial condition\n",
    "\n",
    "    for t in range(1, num_steps):\n",
    "        delay_index = max(t - int(tau / delta_t), 0)\n",
    "        x[t] = x[t - 1] + delta_t * (beta * x[delay_index] / (1.0 + x[delay_index]**n) - gamma * x[t - 1])\n",
    "\n",
    "    return a * x\n",
    "\n",
    "# Simulate the Mackey-Glass equations\n",
    "num_steps = 1000\n",
    "trajectory = mackey_glass(0.2, 0.1, 10, 25, 0.1, 0.2, num_steps)\n",
    "\n",
    "# Plot the simulated trajectory\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(trajectory)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Mackey-Glass')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again try and iterate forward the Mackey-Glass solution by using an ESN autonomously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esn_mackeyglass(steps = 1500, h = 400, Dx = 30, rho = 0.7, gamma = 1, alpha = 0.1, Lambda = 1e-9):\n",
    "    # generate data (rescaled by 10x)\n",
    "    data = mackey_glass(0.2, 0.1, 10, 25, 0.1, 0.2, steps + h)[:,None] * 10.0\n",
    "\n",
    "    # split train and test\n",
    "    Y_train_in = data[:steps-1,:]\n",
    "    Y_train_out = data[1:steps,:]\n",
    "    Y_test = data[steps:,:]\n",
    "\n",
    "    # generate ESN parameters\n",
    "    rng = np.random.default_rng(75871685)\n",
    "    A = rng.normal(loc=0, scale=1, size=[Dx, Dx])\n",
    "    C = rng.normal(loc=0, scale=1, size=[Dx, 1])\n",
    "    zeta = rng.uniform(low=-1, high=1, size=Dx)\n",
    "\n",
    "    # normalize\n",
    "    A = (A / np.linalg.norm(A, 2)) * rho\n",
    "    C = (C / np.linalg.norm(C, 2)) * gamma\n",
    "\n",
    "    # fit esn\n",
    "    W_hat, X, Y_fit, _ = esn_fit(Y_train_out, Y_train_in, Lambda, A, C, zeta, alpha)\n",
    "\n",
    "    # autonomous esn\n",
    "    Xh, Yh = esn_auto(X[-1,:], h, A, C, zeta, alpha, W_hat)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(3, 1, 1)\n",
    "    ax.plot(range(0, steps-1), X, c=\"0.6\")\n",
    "    ax.plot(range(steps-1, steps-1+h), Xh)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN States\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    ax.plot(range(1, steps), Y_train_out, c=\"0.6\", zorder=2)\n",
    "    ax.plot(range(1, steps), Y_fit, c=\"k\", zorder=3)\n",
    "    ax.plot(range(steps, steps+h), Y_test, c=\"C1\", zorder=2)\n",
    "    ax.plot(range(steps, steps+h), Yh, c=\"C2\", zorder=3)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"Mackey-Glass | Autonomous ESN\")\n",
    "    ax.legend([\"$y_t$\", \"ESN Fit\", \"$y_t$ test\", \"ESN Auto\"], loc=\"center left\")\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(esn_mackeyglass, steps=(500, 2500), h=(100, 1000), Dx=(5,50), rho=(0.1, 1.5), gamma=(0.1,10), alpha=(0.,1.),\n",
    "            Lambda=widgets.FloatLogSlider(min=-9, max=2, value=1e-9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move from the deterministic setting to a stochastic one. In particular, we will try to forecast a simple nonlinear autoregressive time series model with an ESN model.\n",
    "\n",
    "Let $\\{\\epsilon_t\\}_{t \\in \\mathbb{Z}}$ be a sequence of independent random variables identically uniformly distributed over the interval $[-1, 1]$, and define\n",
    "$$ X_t = \\frac{2 X_{t-1}}{1 + 0.8\\, X_{t-1}^2} + \\epsilon_t .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE: nonlinear time series\n",
    "def nonlin_ar(n, init=100, seed=12345):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    E = np.zeros(n+init)\n",
    "    X = np.zeros(n+init)\n",
    "    E[0] = rng.uniform(low=-1, high=1)\n",
    "    for t in range(1, n+init):\n",
    "        E[t] = rng.uniform(low=-1, high=1)\n",
    "        X[t] = 2 * X[t-1] / (1 + 0.8*X[t-1]**2) + E[t]\n",
    "    return(X[init:], E[init:])\n",
    "\n",
    "# Generate data\n",
    "X_t, E_t = nonlin_ar(200)\n",
    "\n",
    "# Plot the simulated trajectory\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(X_t)\n",
    "plt.plot(X_t - E_t, c=\"C8\")\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Nonlinear AR(1)')\n",
    "plt.legend([\"$x_t$\", \"$x_t - \\epsilon_t$\"], loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin_ar_condpred(h, X0, mc=1000, seed=54321):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Xh = np.zeros([h, mc])\n",
    "    Xh[0,:] = X0\n",
    "    for t in range(1, h):\n",
    "        # randomly sample innovations\n",
    "        E = rng.uniform(low=-1, high=1, size=mc)\n",
    "        Xh[t,:] = 2 * Xh[t-1,:] / (1 + 0.8*Xh[t-1,:]**2) + E\n",
    "    return(Xh)\n",
    "\n",
    "# Simulate best conditional predictor \n",
    "h = 30\n",
    "condpred = nonlin_ar_condpred(h, X_t[-1], mc=50000)\n",
    "\n",
    "# Plot (with prediction bands)\n",
    "fig = plt.figure(figsize=(6,3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(range(-19, 1), X_t[-20:], c=\"0.6\", zorder=1)\n",
    "ax.plot(range(1, h), np.mean(condpred[1:,], axis=1), c=\"C1\", zorder=3)\n",
    "ax.fill_between(range(1, h), *np.quantile(condpred[1:,], [0.05, 0.95], axis=1), color=\"C1\", alpha=0.2)\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid()\n",
    "ax.set_title(\"Nonlinear Regression\")\n",
    "ax.legend([\"$x_t$\"], loc=\"upper left\")\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esn_nonlinear_ar(steps = 200, h = 30, Dx = 30, rho = 0.7, gamma = 1, theta = 1, alpha = 0.1, Lambda = 1e-6):\n",
    "    # generate data (rescaled by 10x)\n",
    "    X_t, E_t = nonlin_ar(steps+h)\n",
    "    data = X_t[:,None]\n",
    "\n",
    "    # split train and test\n",
    "    X_t_train_in = data[:steps-1,:]\n",
    "    X_t_train_out = data[1:steps,:]\n",
    "    #X_t_test = data[steps:,:]\n",
    "\n",
    "    # generate ESN parameters\n",
    "    rng = np.random.default_rng(75871685)\n",
    "    A = rng.normal(loc=0, scale=1, size=[Dx, Dx])\n",
    "    C = rng.normal(loc=0, scale=1, size=[Dx, 1])\n",
    "    zeta = rng.uniform(low=-1, high=1, size=Dx)\n",
    "\n",
    "    # normalize\n",
    "    A = (A / np.linalg.norm(A, 2)) * rho\n",
    "    C = (C / np.linalg.norm(C, 2)) * gamma\n",
    "    zeta = theta * zeta\n",
    "\n",
    "    # fit esn\n",
    "    W_hat, states, X_t_fit, _ = esn_fit(X_t_train_out, X_t_train_in, Lambda, A, C, zeta, alpha)\n",
    "\n",
    "    # autonomous esn\n",
    "    Xh, Yh = esn_auto(states[-1,:], h, A, C, zeta, alpha, W_hat)\n",
    "\n",
    "    # best conditional predictor \n",
    "    condpred = nonlin_ar_condpred(h+1, X_t_train_out[-1,:], mc=50000)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(3, 1, 1)\n",
    "    ax.plot(range(0, steps-1), states, c=\"0.6\")\n",
    "    ax.plot(range(steps-1, steps-1+h), Xh)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"ESN States\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    ax.plot(range(1, steps+h), X_t[1:,], c=\"C0\", alpha=0.4, zorder=2)\n",
    "    ax.plot(range(1, steps+h), X_t[1:,] - E_t[1:], c=\"C8\", zorder=2)\n",
    "    ax.plot(range(1, steps), X_t_fit, c=\"k\", zorder=3)\n",
    "    ax.plot(range(steps, steps+h), np.mean(condpred[1:,], axis=1), c=\"C1\", zorder=3)\n",
    "    ax.fill_between(range(steps, steps+h), *np.quantile(condpred[1:,], [0.05, 0.95], axis=1), color=\"C1\", alpha=0.2)\n",
    "    ax.plot(range(steps, steps+h), Yh, c=\"C2\", zorder=3)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid()\n",
    "    ax.set_title(\"Mackey-Glass | Autonomous ESN\")\n",
    "    ax.legend([\"$x_t$\", \"$x_t - \\epsilon_t$\", \"ESN Fit\", \"Best Cond Pred\"], loc=\"center left\")\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(esn_nonlinear_ar, steps=(50, 500), h=(3, 100), Dx=(5,50), rho=(0.1, 1.5), gamma=(0.1,5), theta=(0.0, 1.0), alpha=(0.,1.),\n",
    "            Lambda=widgets.FloatLogSlider(min=-9, max=2, value=1e-9));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESNExperiments-LmEhiHNj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
